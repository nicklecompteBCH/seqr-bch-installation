#!/usr/bin/env python2.7

import argparse
import logging
import os
import sys
from getpass import getpass

from deploy.servctl_utils.deploy_command_utils import deploy, DEPLOYABLE_COMPONENTS, COMPONENT_GROUP_NAMES, \
    DEPLOYMENT_TARGETS, resolve_component_groups
from deploy.servctl_utils.loading_command_utils import load_example_project, update_reference_data, load_dataset
from deploy.servctl_utils.other_command_utils import print_log, port_forward, create_user, show_dashboard, \
    show_status, wait_for, troubleshoot_component, delete_component, reset_database, delete_all, \
    get_component_port_pairs, check_kubernetes_context, set_environment, open_shell_in_component, \
    copy_files_to_or_from_pod

logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

SET_ENVIRONMENT_COMMAND = 'set-env'
STATUS_COMMAND = 'status'
DEPLOY_COMMAND = 'deploy'
DEPLOY_ALL_COMMAND = 'deploy-all'
UPDATE_REFERENCE_DATA_COMMAND = 'update-reference-data'
LOAD_EXAMPLE_PROJECT_COMMAND = 'load-example-project'
LOAD_DATASET_COMMAND = 'load-dataset'
LOG_COMMAND = 'logs'
TROUBLESHOOT_COMMAND = 'troubleshoot'
PORT_FORWARD_COMMAND = 'port-forward'
PORT_FORWARD_AND_LOG_COMMAND = 'connect-to'
COPY_TO_POD_COMMAND = 'copy-to'
COPY_FROM_POD_COMMAND = 'copy-from'
SHELL_COMMAND = 'shell'
DASHBOARD_COMMAND = 'dashboard'
DELETE_COMMAND = 'delete'
DELETE_ALL_COMMAND = 'delete-all'
RESET_COMMAND = 'reset-database'
CREATE_USER_COMMAND = 'create-user'


p = argparse.ArgumentParser()
subparsers = p.add_subparsers(dest='command')

COMPONENT_ARG_CHOICES = DEPLOYABLE_COMPONENTS + COMPONENT_GROUP_NAMES
DEPLOYMENT_TARGETS_SET = set(DEPLOYMENT_TARGETS.keys())

## ENVIRONMENT_COMMAND
sp = subparsers.add_parser(SET_ENVIRONMENT_COMMAND, description="Set terminal environment for a particular deployment target (eg. 'minikube', 'gcloud-dev', etc.)")
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## LOAD_DATASET_COMMAND
sp = subparsers.add_parser(LOAD_DATASET_COMMAND, description="Load new dataset into a new or existing project in seqr.")
sp.add_argument("-g", "--genome-version", help="Genome version 37 or 38", choices=["37", "38"], required=True)
sp.add_argument("--project-guid", help="seqr project id", required=True)
sp.add_argument("--sample-type", help="Sample type", choices=["WES", "WGS", "RNA"], required=True)
sp.add_argument("--dataset-type", help="Dataset type", choices=["VARIANTS", "SV"], required=True)
sp.add_argument('--remap-sample-ids', help="File path containing 2 tab-separated columns: current sample id and desired sample id")
sp.add_argument('--subset-samples', help="File path containing ids for samples to keep; if used with --remap-sample-ids, ids are the desired ids (post remapping)")
sp.add_argument("--ignore-extra-sample-ids-in-tables", action="store_true")
sp.add_argument("--ignore-extra-sample-ids-in-vds", action="store_true")

sp.add_argument("--fam-file", help=".fam file used to check VDS sample IDs and assign samples to indices with a max of "
    "'num_samples' per index, but making sure that samples from the same family don't end up in different indices. "
    "If used with --remap-sample-ids, contains IDs of samples after remapping")
sp.add_argument("--start-with-step", help="Which pipeline step to start with.", type=int, default=0, choices=[0, 1, 2, 3, 4])
sp.add_argument("--stop-after-step", help="Pipeline will exit after this stesp.", type=int, choices=[0, 1, 2, 3, 4])
sp.add_argument("--output-vds", help="(optional) Output vds filename prefix (eg. test-vds)")

sp.add_argument("--vep-block-size", help="Block size to use for VEP", default=200, type=int)
sp.add_argument("--es-block-size", help="Block size to use when exporting to elasticsearch", default=1000, type=int)
sp.add_argument("--cpu-limit", help="Max CPUs to use when loading the dataset. Only applies to local deployments.", type=int)
sp.add_argument("--input-vcf", help="Local path or google cloud bucket path (eg. gs://) of VCF to load", required=True)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="which kubernetes cluster to load the data to")

# LOAD_EXAMPLE_PROJECT_COMMAND
sp = subparsers.add_parser(LOAD_EXAMPLE_PROJECT_COMMAND, description="Load a demo project with an example dataset")
sp.add_argument("-g", "--genome-version", help="Genome version 37 or 38", choices=["37", "38"], default="37")
sp.add_argument("--cpu-limit", help="Max CPUs to use when loading the dataset. Only applies to local deployments.", type=int)
sp.add_argument("--start-with-step", help="Which pipeline step to start with.", type=int, default=0, choices=[0, 1, 2, 3, 4])
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

# UPDATE_REFERENCE_DATA_COMMAND
sp = subparsers.add_parser(UPDATE_REFERENCE_DATA_COMMAND, description="Load or update gene-level and other reference data (eg. gencode, omim, HPO, etc.)")
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## LOG_COMMAND
sp = subparsers.add_parser(LOG_COMMAND, description="Show logs for one or more components")
sp.add_argument("-f", "--stream-log", action="store_true", help="continue printing new logs - similar to tail -f")
sp.add_argument("--previous", action="store_true", help="Prints logs from a previous instance of the container. "
    "This is useful for debugging pods that don't start or immediately enter crash-loop.")
sp.add_argument("components", nargs="+", help="show log", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## TROUBLESHOOT_COMMAND
sp = subparsers.add_parser(TROUBLESHOOT_COMMAND, description="Print detailed debugging info to troubleshoot components not starting")
sp.add_argument("component", help="print detailed debugging info", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## PORT_FORWARD_COMMAND
sp = subparsers.add_parser(PORT_FORWARD_COMMAND, description="Start port-forwarding for service(s) running inside one or more containers, "
    "allowing connections via localhost. After starting port-forwarding as a background process, start streaming logs from these components. ")
sp.add_argument("components", nargs="+", help="start port-forwarding for service(s) running in the given component container(s), allowing connections via localhost", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## PORT_FORWARD_AND_LOG_COMMAND
sp = subparsers.add_parser(PORT_FORWARD_AND_LOG_COMMAND, description="Start port-forwarding for service(s) running inside one or more containers, allowing connections via localhost")
sp.add_argument("components", nargs="+", help="start port-forwarding for service(s) running in the given component container(s), allowing connections via localhost", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## COPY_COMMAND
for command in [COPY_TO_POD_COMMAND, COPY_FROM_POD_COMMAND]:
    sp = subparsers.add_parser(command, description="Copy files to or from a container")
    sp.add_argument("component", help="open a bash shell inside this component container", choices=COMPONENT_ARG_CHOICES)
    sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

    if command == COPY_TO_POD_COMMAND:
        sp.add_argument("source_path", help="source file path")
        sp.add_argument("dest_path", nargs="?", help="destination file path", default="/root")
    elif command == COPY_FROM_POD_COMMAND:
        sp.add_argument("source_path", help="source file path")
        sp.add_argument("dest_path", nargs="?", help="destination file path", default=".")

## SHELL_COMMAND
sp = subparsers.add_parser(SHELL_COMMAND, description="Open a bash shell inside one of the component containers")
sp.add_argument("component", help="open a bash shell inside this component container", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## CREATE_USER_COMMAND
sp = subparsers.add_parser(CREATE_USER_COMMAND, description="Create a new seqr superuser account.")
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## STATUS_COMMAND
sp = subparsers.add_parser(STATUS_COMMAND, description="Print docker and kubectl info")
sp.add_argument("deployment_target", nargs="?", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## DASHBOARD_COMMAND
sp = subparsers.add_parser(DASHBOARD_COMMAND, description="Show the kubernetes dashboard")
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## DELETE_COMMAND
sp = subparsers.add_parser(DELETE_COMMAND, description="Terminate any live deployments, services, and pods for a given component")
sp.add_argument("components", nargs="+", help="terminate all deployments, services, and pods for the given component(s)", choices=COMPONENT_ARG_CHOICES)
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## DELETE_ALL_COMMAND
sp = subparsers.add_parser(DELETE_ALL_COMMAND, description="Delete all components + the cluster")
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")

## RESET_COMMAND
sp = subparsers.add_parser(RESET_COMMAND, description="Delete all data from the given database")
sp.add_argument('database', nargs="+", choices=["seqrdb", "phenotipsdb", "mongodb"])
sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="kubernetes cluster")


def _check_restore_db_file(path):
    if not os.path.isfile(path):
        p.error("File doesn't exist: %s" % path)
    return os.path.abspath(path)


for command_name in [DEPLOY_COMMAND, DEPLOY_ALL_COMMAND]:
    sp = subparsers.add_parser(command_name, description="Deploy one or more components")
    sp.add_argument("-d", "--delete-before-deploy", action="store_true", help="run 'kubectl delete' on component(s) before deploying them")
    sp.add_argument("-b", "--build-docker-images", action="store_true", help="build the docker image for each component before deploying it. ")
    sp.add_argument("-f", "--force", action="store_true", help="same as -b and -d together. Also, forces docker image to be rebuilt from the beginning with --no-cache.")
    g = sp.add_mutually_exclusive_group()
    g.add_argument("-p", "--push-to-registry", action="store_true", help="push local docker image to a docker registry. The destination registry is determined by the "
        "DOCKER_IMAGE_PREFIX value in shared-settings.yaml. "
        "When deploying to minikube, specifying -b and not -p will build the docker image using minikube's docker instance. This will allow the image "
        "to be reused in local minikube deployments as long as this minikube instance exists. "
        "Adding -p pushes the image to a remote registry, and allows it to be reused by remote minikube installations and in local deploys even "
        "if this minikube instance is destroyed and recreated."
        "When deploying to gcloud-dev or gcloud-prod, -p is necessary to make an image available because google container engine clusters "
        "can only use images from the google container registry and/or public registries like dockerhub.")
    g.add_argument("--only-push-to-registry", action="store_true", help="push local docker image to a docker registry and then stop, without deploying the component(s) to kubernetes.")
    sp.add_argument("-t", "--docker-image-tag", help="deploy docker image(s) with this tag. If -b also used, the built docker images will be tagged with this.")
    sp.add_argument("--reset-db", action="store_true", help="if deploying phenotips or seqr, delete and re-initialize their postgres databases")
    sp.add_argument("--restore-seqr-db-from-backup",
        help="if deploying seqr, this option can be used to specify a SQL database backup from "
            "which to restore the database. This backup file can be generated by running "
            "`pg_dump -U postgres seqrdb | gzip -c - > seqrdb_backup.sql.gz`", type=_check_restore_db_file)
    sp.add_argument("--restore-phenotips-db-from-backup",
        help="if deploying phenotips, this option can be used to specify a SQL database backup from "
            "which to restore the database. This backup file can be generated by running "
            "`pg_dump -U postgres xwiki  | gzip -c - > phenotipsdb_backup.sql.gz`", type=_check_restore_db_file)

    if command_name == DEPLOY_COMMAND:
        sp.add_argument("components", nargs="+", help="one or more components to deploy", choices=COMPONENT_ARG_CHOICES)

    sp.add_argument("deployment_target", choices=DEPLOYMENT_TARGETS_SET, help="which kubernetes cluster to deploy to")


args = p.parse_args()

# process command-line args
if args.deployment_target and args.command != SET_ENVIRONMENT_COMMAND:
    commands_that_call_init_cluster = set([
        DEPLOY_ALL_COMMAND,
    ])

    commands_that_dont_modify_kubernetes_state = set([
        SHELL_COMMAND,
        PORT_FORWARD_COMMAND,
        PORT_FORWARD_AND_LOG_COMMAND,
        LOG_COMMAND,
        DASHBOARD_COMMAND,
    ])

    if args.command not in commands_that_call_init_cluster and (args.command != DEPLOY_COMMAND or (args.components and "init-cluster" not in args.components)):
        set_context = (args.command in commands_that_dont_modify_kubernetes_state)
        check_kubernetes_context(args.deployment_target, set_if_different=set_context)

runtime_settings = {}
runtime_settings["BASE_DIR"] = os.path.abspath(os.path.dirname(__file__))

if args.command in [DEPLOY_COMMAND, DEPLOY_ALL_COMMAND]:
    # transfer args to runtime_settings
    runtime_settings['FORCE_BUILD_DOCKER_IMAGES'] = bool(args.force)
    runtime_settings["DELETE_BEFORE_DEPLOY"] = True if args.delete_before_deploy or args.force else None
    runtime_settings["BUILD_DOCKER_IMAGES"] = True if args.build_docker_images or args.force else None
    runtime_settings["RESTORE_PHENOTIPS_DB_FROM_BACKUP"] = args.restore_phenotips_db_from_backup
    runtime_settings["RESTORE_SEQR_DB_FROM_BACKUP"] = args.restore_seqr_db_from_backup
    runtime_settings["PUSH_TO_REGISTRY"] = bool(args.push_to_registry) or bool(args.only_push_to_registry)
    runtime_settings["ONLY_PUSH_TO_REGISTRY"] = bool(args.only_push_to_registry)
    runtime_settings["DOCKER_IMAGE_TAG"] = args.docker_image_tag
    runtime_settings["RESET_DB"] = bool(args.reset_db)

    #email = password = None
    if args.command == DEPLOY_COMMAND:
        components_to_deploy = resolve_component_groups(args.deployment_target, args.components)

    elif args.command == DEPLOY_ALL_COMMAND:
        # compute which components to deploy
        components_to_deploy = DEPLOYMENT_TARGETS[args.deployment_target]

        #print("\nOptionally enter an email and password for the django/seqr superuser account. After seqr is deployed, "
        #      "you will be able to login to seqr with this account. Leave blank to skip.")

        #email = raw_input("(optional) seqr superuser email: ")
        #if email:
        #    password = getpass("seqr superuser password (hidden when typed): ")

    deploy(args.deployment_target, components=components_to_deploy, runtime_settings=runtime_settings)

    logger.info("==> Deployed " + ", ".join(components_to_deploy))
    #if email:
    #    create_user(args.deployment_target, email, password)

    if not runtime_settings["ONLY_PUSH_TO_REGISTRY"]:
        if args.command == DEPLOY_ALL_COMMAND:
            if "seqr" in components_to_deploy:
                update_reference_data(args.deployment_target)
                #load_example_project(args.deployment_target)

                components_to_connect_to = ["seqr"]
            elif "es-kibana" in components_to_deploy:
                components_to_connect_to = ["es-kibana"]
            else:
                components_to_connect_to = []
        else:
            components_to_connect_to = components_to_deploy

        component_port_pairs = get_component_port_pairs(components_to_connect_to)
        if component_port_pairs:
            logger.info("=============")
            print_los_for_components = [pair[0] for pair in component_port_pairs]
            logger.info("tail %s logs ..." % components_to_connect_to)
            os.system("sleep 10")

            procs1 = port_forward(component_port_pairs, deployment_target=args.deployment_target, wait=False, open_browser=True)
            procs2 = print_log(components_to_connect_to, deployment_target=args.deployment_target, enable_stream_log=True, wait=False)
            wait_for(procs1 + procs2)

    logger.info("==> Done")
elif args.command == UPDATE_REFERENCE_DATA_COMMAND:
    update_reference_data(args.deployment_target)

elif args.command == LOAD_EXAMPLE_PROJECT_COMMAND:
    load_example_project(args.deployment_target, genome_version=args.genome_version, cpu_limit=args.cpu_limit, start_with_step=args.start_with_step)

elif args.command == LOAD_DATASET_COMMAND:
    load_dataset(
        args.deployment_target,
        args.project_guid,
        args.genome_version,
        args.sample_type,
        args.dataset_type,
        args.input_vcf,
        remap_sample_ids=args.remap_sample_ids,
        subset_samples=args.subset_samples,
        ignore_extra_sample_ids_in_tables=args.ignore_extra_sample_ids_in_tables or None,
        ignore_extra_sample_ids_in_vds=args.ignore_extra_sample_ids_in_vds or None,
        fam_file=args.fam_file,
        start_with_step=args.start_with_step,
        stop_after_step=args.stop_after_step,
        output_vds=args.output_vds,
        vep_block_size=args.vep_block_size,
        es_block_size=args.es_block_size,
        cpu_limit=args.cpu_limit)

elif args.command == LOG_COMMAND:
    components = resolve_component_groups(args.deployment_target, args.components)
    print_log(components, args.deployment_target, args.stream_log, previous=args.previous)

elif args.command == PORT_FORWARD_COMMAND:
    component_port_pairs = get_component_port_pairs(resolve_component_groups(args.deployment_target, args.components))
    port_forward(component_port_pairs, deployment_target=args.deployment_target)

elif args.command == PORT_FORWARD_AND_LOG_COMMAND:
    component_port_pairs = get_component_port_pairs(resolve_component_groups(args.deployment_target, args.components))
    if component_port_pairs:
        procs1 = port_forward(component_port_pairs, deployment_target=args.deployment_target, wait=False, open_browser=True)
        procs2 = print_log(resolve_component_groups(args.deployment_target, args.components), args.deployment_target, enable_stream_log=True, wait=False)

        wait_for(procs1 + procs2)

elif args.command in [COPY_FROM_POD_COMMAND, COPY_TO_POD_COMMAND]:
    direction = 1 if args.command == COPY_TO_POD_COMMAND else -1
    copy_files_to_or_from_pod(args.component, args.deployment_target, args.source_path, args.dest_path, direction=direction)

elif args.command == SHELL_COMMAND:
    open_shell_in_component(args.component, args.deployment_target)

elif args.command == SET_ENVIRONMENT_COMMAND:
    set_environment(args.deployment_target)

elif args.command == CREATE_USER_COMMAND:
    create_user(args.deployment_target)

elif args.command == STATUS_COMMAND:
    show_status()

elif args.command == DASHBOARD_COMMAND:
    show_dashboard()

elif args.command == DELETE_COMMAND:
    for component in resolve_component_groups(args.deployment_target, args.components):
        delete_component(component, deployment_target=args.deployment_target)

elif args.command == RESET_COMMAND:
    reset_database(args.database, deployment_target=args.deployment_target)

elif args.command == TROUBLESHOOT_COMMAND:
    troubleshoot_component(args.component, deployment_target=args.deployment_target)

elif args.command == DELETE_ALL_COMMAND:
    delete_all(args.deployment_target)

else:
    p.error("Unexpected command: " + str(args.command))